{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer json No sub array to json multi row file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import json_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dl.cfg']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['AWS_ACCESS_KEY_ID'] = config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = config['AWS']['AWS_SECRET_ACCESS_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = '2008'\n",
    "source_folder = 'league_fixtures/'\n",
    "dest_folder = f'league_fixtures/{year}/'\n",
    "backup_folder = f'backup/league_fixtures/{year}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client('s3')\n",
    "#s3_resource = boto3.resource('s3')\n",
    "bucket_name='capstonefootballbucket'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if is need to flatten file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_content(file_content):\n",
    "    # Check if the content starts with an array or contains a newline character\n",
    "    return file_content.strip().startswith('[') or '\\n' not in file_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start transfer JSON array to JSON rows file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_key=league_fixtures/2008/4_2008fixtures.json\n",
      "backup_key=backup/league_fixtures/2008/4_2008fixtures.json\n"
     ]
    }
   ],
   "source": [
    "paginator = s3_client.get_paginator(\"list_objects_v2\")\n",
    "response = paginator.paginate(Bucket=bucket_name,  Prefix=source_folder, Delimiter='/', PaginationConfig={\"PageSize\": 300})\n",
    "count = 0\n",
    "for page in response:\n",
    "    files = page.get(\"Contents\")\n",
    "    for file in files:\n",
    "        key = file['Key']\n",
    "        if key.endswith('/'):\n",
    "            continue\n",
    "        if not year in key:\n",
    "            continue\n",
    "        #print(f\"file path = {key}\")\n",
    "        #file_name = os.path.basename(key)\n",
    "        # read original JSON file\n",
    "        file_obj = s3_client.get_object(Bucket=bucket_name, Key=key)\n",
    "        file_content = file_obj[\"Body\"].read().decode('utf-8')\n",
    "        \n",
    "        # Check if file_content is empty or None\n",
    "        if not file_content:\n",
    "            continue\n",
    "        #check if file_content is starts with an array or not contains a newline character    \n",
    "        if not is_valid_content(file_content):\n",
    "            continue\n",
    "            \n",
    "        data = json.loads(file_content)\n",
    "        flattened_data = pd.DataFrame()    \n",
    "        for item in data:\n",
    "            if isinstance(item, str):\n",
    "                try:\n",
    "                    item = json.loads(item)\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"unparseable JSON string. file_key={key}\")\n",
    "                    break\n",
    "                    \n",
    "            # First, flatten the nested JSON objects using pd.json_normalize()\n",
    "            flattened_item_df = json_normalize(item, sep='_')           \n",
    "            flattened_data = pd.concat([flattened_data, flattened_item_df], ignore_index=True)\n",
    "\n",
    "        json_lines = flattened_data.to_json(orient='records', lines=True)\n",
    "        #build target object path\n",
    "        target_key = key.replace(source_folder, dest_folder)\n",
    "        print(f\"target_key={target_key}\")\n",
    "        # write to new file to destination folder\n",
    "        s3_client.put_object(Bucket=bucket_name, Key=target_key, Body=json_lines)\n",
    "        #build backup object path\n",
    "        backup_key = key.replace(source_folder, backup_folder)\n",
    "        print(f\"backup_key={backup_key}\")\n",
    "        #move target object\n",
    "        s3_client.copy_object(Bucket=bucket_name, CopySource=f\"{bucket_name}/{key}\", Key=backup_key)\n",
    "        #verify that the tartet object exists\n",
    "        waiter = s3_client.get_waiter('object_exists')\n",
    "        waiter.wait(Bucket=bucket_name, Key=backup_key)\n",
    "        \n",
    "        s3_client.delete_object(Bucket=bucket_name, Key=key)\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
